import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
import librosa
import librosa.display
from IPython.display import Audio
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, BatchNormalization

drive.mount('/content/drive')

Tess = '/content/drive/MyDrive/Tess'

paths = []
labels = []
for dirname, _, filenames in os.walk(Tess):
    for filename in filenames:
        paths.append(os.path.join(dirname, filename))
        label = filename.split('_')[-1]
        label = label.split('.')[0]
        labels.append(label.lower())
    if len(paths) == 2800:
        break


df = pd.DataFrame()
df['speech'] = paths
df['label'] = labels

le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

sns.countplot(data=df, x='label')

def extract_mfcc(filename):
    y, sr = librosa.load(filename, duration=3, offset=0.5)
    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    return mfcc

X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))

X = [x for x in X_mfcc]
X = np.array(X)
X = np.expand_dims(X, -1)

# Use label_encoded for training
y = df['label_encoded'].values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# # Model architecture
# model = Sequential([
#     LSTM(256, return_sequences=True, input_shape=(40, 1)),
#     BatchNormalization(),
#     Dropout(0.5),
#     LSTM(128, return_sequences=True),
#     BatchNormalization(),
#     Dropout(0.5),
#     LSTM(64),
#     BatchNormalization(),
#     Dropout(0.5),
#     Dense(64, activation='relu'),
#     Dropout(0.5),
#     Dense(7, activation='softmax')
# ])

# # Compile the model
# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model.summary()

# # Train the model
# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64)







from keras.callbacks import EarlyStopping

# Model architecture
model = Sequential([
    LSTM(256, return_sequences=True, input_shape=(40, 1)),
    BatchNormalization(),
    Dropout(0.5),
    LSTM(128, return_sequences=True),
    BatchNormalization(),
    Dropout(0.5),
    LSTM(64),
    BatchNormalization(),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])









early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=64, callbacks=[early_stopping])

